# -*- coding: utf-8 -*-
"""Language Detection .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PkDwLcNdazGMkETjdi-y6Rzz8S-1crrm
"""

#Implementation
#Importing libraries and dataset

#import all the libraries like pandas ,string,numpy,re,matplotlib and seaborn

import string                 #strings in Python are arrays of bytes representing unicode characters.
import pandas as pd           #Pandas is a Python library used for working with data sets
import numpy as np            #NumPy is a Python library used for working with arrays.
import re                     #A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern. eg. findall,search,split,sub
import matplotlib.pyplot as plt      #Matplotlib is a low level graph plotting library in python that serves as a visualization utility.
import seaborn as sns                #Seaborn is a library that uses Matplotlib underneath to plot graphs. It will be used to visualize random distributions.

#read the csv file
# Loading the dataset

df=pd.read_csv("C:\\Users\\User\\Downloads\\Language Detection.csv~\\Language Detection.csv")
df

#there are 10337 rows × 2 columns
#In this datasets there are text and language. In this all over languages like english,hindi,french,arabic,kannada,tamil,telugu,gurjati etc.

#As I told you earlier this dataset contains text details for 17 different languages. So let’s count the value count for each language.

df["Language"].value_counts()

#here we print top 5 row using this head method

df.head()

#this is top 5 rows from datasets

#we can also print the last 5 rows using this method which tail.

df.tail()

#Text Preprocessing
#This is a dataset created using scraping the Wikipedia, so it contains many unwanted symbols,
#numbers which will affect the quality of our model. So we should perform text preprocessing techniques.
#Now, In this part we remove the punctuation like comma,semicolon,full stop.
#here we create def fuction and apply for loop and remove all the unneccessary information like punctuation.

def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, '')
    return text

#In this we create df variable and pass the varibale text

df["Text"] = df["Text"].apply(remove_punctuations)
df["Text"]

#after applying thsi method we got data neat and clean which is remove all the punctuation.

# sklearn.model_selection a module that provides tools for splitting datasets into training and testing sets, performing cross-validation,
# and generally managing the process of selecting the best model parameters for machine learning tasks

#The train_test_split() method is used to split our data into train and test sets

from sklearn.model_selection import train_test_split

#Separating Independent and Dependent features
#Now we can separate the dependent and independent variables, here text data is the independent variable and
#the language name is the dependent variable.

X=df.iloc[:,0]
Y=df.iloc[:,1]

X

Y

#Train Test Splitting
#We preprocessed our input and output variable. The next step is to create the training set, for training the model and test set,
#for evaluating the test set. For this process, we are using a train test split

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=.2)

X_train,X_test,Y_train,Y_test
#X_train

X_test

Y_train

Y_test

#Model Training and Prediction
#The sklearn. feature_extraction module can be used to extract features in a format supported by machine learning
#algorithms from datasets consisting of formats such as text and image.

from sklearn import feature_extraction

#Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the
#importance of a term to a document in the corpus. Denote a term by t , a document by d , and the corpus by D .

vec=feature_extraction.text.TfidfVectorizer(ngram_range=(1,2),analyzer='char')

#linear_model is a class of the sklearn module if contain different functions for performing machine learning with linear models

from sklearn import pipeline
from sklearn import linear_model

model_pipe=pipeline.Pipeline([('vec',vec),('clf',linear_model.LogisticRegression())])

#model fitting

model_pipe.fit(X_train,Y_train)

#clases

model_pipe.classes_

# Prediction

predict_val=model_pipe.predict(X_test)

#Model Evaluation

#import metrics
from sklearn import metrics

#Predict the accuracy

metrics.accuracy_score(Y_test,predict_val)*100

#confusion matrix A confusion matrix is a table that is used to define the performance of a classification algorithm.
#A confusion matrix visualizes and summarizes the performance of a classification algorithm.

metrics.confusion_matrix(Y_test,predict_val)

#Model testing
model_pipe.predict(["माझे नाव पूजा आहे"])

#It is predicted the value

model_pipe.predict(["வாருங்கம் "])

import pickle

new_file=open('model.pckl','wb')
pickle.dump(model_pipe,new_file)
new_file.close()

import os

os

ls

os.listdir()